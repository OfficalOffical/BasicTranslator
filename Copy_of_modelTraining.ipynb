{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OfficalOffical/BasicTranslator/blob/master/Copy_of_modelTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.src.regularizers import l2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-16T16:01:35.935353Z",
          "start_time": "2024-01-16T16:01:30.626740Z"
        },
        "id": "abbbc9eb7cfb3880"
      },
      "id": "abbbc9eb7cfb3880",
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import zscore\n",
        "import re\n",
        "\n",
        "\n",
        "def work_on_outliers(input_df,inf_threshold,sup_threshold, feature_number,visualize):\n",
        "    # WORKING ON OUTLIERS\n",
        "\n",
        "    # Working on negpmax feature (the only one that is showing outliers)\n",
        "    features = [f for f in input_df.columns[feature_number::4]]\n",
        "    if visualize:\n",
        "        fig, axes = plt.subplots(4, 3, figsize=(15, 10))  # Adjust the figsize as needed\n",
        "\n",
        "        # Flatten the 2D array of subplots\n",
        "        axes = axes.flatten()\n",
        "\n",
        "    output_df=input_df.copy()\n",
        "\n",
        "    for i, feature in enumerate(features):\n",
        "        Q1 = input_df[feature].quantile(0.25)\n",
        "        Q3 = input_df[feature].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - inf_threshold * IQR\n",
        "\n",
        "        if feature_number==3:\n",
        "            upper_bound=0\n",
        "        else:\n",
        "            upper_bound=Q3 + sup_threshold * IQR\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # removing too  small values and positive values for negpmax\n",
        "        #removing too large values for rms\n",
        "\n",
        "        outliers = output_df[(output_df[feature] < lower_bound) | (output_df[feature] > upper_bound)]\n",
        "\n",
        "\n",
        "\n",
        "        output_df = output_df[(output_df[feature] > lower_bound) & (output_df[feature] < upper_bound)]\n",
        "        if visualize:\n",
        "            axes[i].scatter(outliers[\"x\"], outliers[\"y\"], c=outliers[feature], cmap='viridis')  # Adjust the colormap as needed\n",
        "            axes[i].set_title(f'REMOVED OUTLIERS: {feature}')\n",
        "            axes[i].set_xlabel(\"X-axis\")\n",
        "            axes[i].set_ylabel(\"Y-axis\")\n",
        "\n",
        "    if visualize:\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    if visualize:\n",
        "\n",
        "        # let's see if the representation is working better after removing the outliers '\n",
        "        random_10_percent = output_df.sample(frac=0.1, random_state=42)\n",
        "\n",
        "        fig, axes = plt.subplots(4, 3, figsize=(15, 10))  # Adjust the figsize as needed\n",
        "        fig.suptitle(\"RESULT AFTER REMOVING OUTLIERS (ON A 10% SAMPLE) \", fontsize=16)\n",
        "        # Flatten the 2D array of subplots\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        for i, feature in enumerate(features):\n",
        "            axes[i].scatter(random_10_percent[\"x\"], random_10_percent[\"y\"], c=random_10_percent[feature],\n",
        "                            cmap='viridis',label=feature)  # Adjust the colormap as needed\n",
        "            axes[i].set_title(f'scatter plot: {feature}')\n",
        "            axes[i].set_xlabel(\"X-axis\")\n",
        "            axes[i].set_ylabel(\"Y-axis\")\n",
        "            axes[i].legend(loc='best')  # Add legend\n",
        "\n",
        "        # Adjust layout to prevent overlap\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    return output_df\n",
        "\n",
        "\n",
        "#MAIN CODE\n",
        "def pre_process(df, normalize=False, see_graphs=False):\n",
        "    #REMOVING NOISE COLUMNS\n",
        "\n",
        "    #working on tmax features, which seem to be the most meaningful for removing the noise sensors\n",
        "    z_scores = pd.DataFrame(zscore(df), columns=df.columns[5::5])\n",
        "    outlier_threshold = 3\n",
        "    outliers = (z_scores.abs() > outlier_threshold)\n",
        "\n",
        "    #noise sensors have a random distribution of tmax, almost all  the values are contained\n",
        "    #in the value of 3*dev_std. We order by increasing order of outliers and remove the columns\n",
        "    #that have less outliers (random distribution)\n",
        "\n",
        "    percentage_outliers = (outliers.sum() / len(df)) * 100\n",
        "    sorted_columns = percentage_outliers.sort_values()\n",
        "    numbers_to_remove = [re.search(r'\\[(\\d+)\\]', column).group(1) for column in sorted_columns[:6].index]\n",
        "    columns_to_drop = [column for column in df.columns if\n",
        "                       any(re.search(rf'\\[{number}\\]', column) for number in numbers_to_remove)]\n",
        "\n",
        "    df_clean = df.drop(columns=columns_to_drop)\n",
        "\n",
        "    # the feature describing the area and the feature describing the pmax are highly correlated,\n",
        "    # we can remove the area feature.\n",
        "\n",
        "    features_area = [f for f in df_clean.columns[4::5]]\n",
        "\n",
        "    df_clean = df_clean.drop(columns=features_area)\n",
        "\n",
        "    #OUTLIERS\n",
        "\n",
        "    df_clean_out_negpmax=work_on_outliers(df_clean,25,0,3,see_graphs) #3 means negpmax\n",
        "\n",
        "    #we can work also on the rms\n",
        "\n",
        "    df_clean_out_rms = work_on_outliers(df_clean, 1.5,1.5, 5,see_graphs)  # 5 means rms\n",
        "\n",
        "    # SPLITTING AND NORMALIZING IF REQUIRED\n",
        "    df_final=df_clean_out_rms.copy()\n",
        "    X=df_final.iloc[:,2:]\n",
        "    y=df_final.iloc[:,:2]\n",
        "\n",
        "    if normalize:\n",
        "        scaler = StandardScaler()\n",
        "\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "    else:\n",
        "        X_scaled= X.copy()\n",
        "\n",
        "\n",
        "\n",
        "    return df_final\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uvyHvOS6e3yI"
      },
      "id": "uvyHvOS6e3yI",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.losses import Loss\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def euclidean_distance_loss(y_true, y_pred):\n",
        "    return K.sqrt(K.sum(K.square(y_pred - y_true), axis=-1))\n",
        "\n",
        "def euclidean_distance_error(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the mean Euclidean distance error between true and predicted 2D points.\n",
        "    :param y_true: Array of true values with shape (n_samples, 2).\n",
        "    :param y_pred: Array of predicted values with shape (n_samples, 2).\n",
        "    :return: Mean Euclidean distance error.\n",
        "    \"\"\"\n",
        "    return np.mean(np.sqrt(np.sum(np.square(y_true - y_pred), axis=1)))\n",
        "\n"
      ],
      "metadata": {
        "id": "I4T3qFpNfYJa"
      },
      "id": "I4T3qFpNfYJa",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(347342, 50)\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('development.csv')\n",
        "df = pre_process(df)\n",
        "print(df.shape)\n",
        "#get random 20 k samples\n",
        "\n",
        "\n",
        "\n",
        "# Prepare the dataset\n",
        "X = df.iloc[:, 2:].values\n",
        "Y = df.iloc[:, :2].values"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-16T16:14:32.875274Z",
          "start_time": "2024-01-16T16:14:28.077496Z"
        },
        "id": "369b0142b2aa19ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a51f7624-e63e-4582-daef-0f566cca8169"
      },
      "id": "369b0142b2aa19ad",
      "execution_count": 24
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_101 (Dense)           (None, 128)               6272      \n",
            "                                                                 \n",
            " dense_102 (Dense)           (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_103 (Dense)           (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_104 (Dense)           (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16674 (65.13 KB)\n",
            "Trainable params: 16674 (65.13 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "8684/8684 [==============================] - 21s 2ms/step - loss: 2997.4817\n",
            "Epoch 2/50\n",
            "8684/8684 [==============================] - 19s 2ms/step - loss: 928.5172\n",
            "Epoch 3/50\n",
            "8684/8684 [==============================] - 20s 2ms/step - loss: 159.2505\n",
            "Epoch 4/50\n",
            "8684/8684 [==============================] - 19s 2ms/step - loss: 137.3962\n",
            "Epoch 5/50\n",
            "8684/8684 [==============================] - 21s 2ms/step - loss: 184.9814\n",
            "Epoch 6/50\n",
            "8684/8684 [==============================] - 19s 2ms/step - loss: 260.5446\n",
            "Epoch 7/50\n",
            "8684/8684 [==============================] - 20s 2ms/step - loss: 304.9594\n",
            "Epoch 8/50\n",
            "8684/8684 [==============================] - 21s 2ms/step - loss: 151.0211\n",
            "Epoch 9/50\n",
            "8684/8684 [==============================] - 21s 2ms/step - loss: 144.6646\n",
            "Epoch 10/50\n",
            "8684/8684 [==============================] - 20s 2ms/step - loss: 220.7315\n",
            "Epoch 11/50\n",
            "8684/8684 [==============================] - 21s 2ms/step - loss: 113.1442\n",
            "Epoch 12/50\n",
            "8684/8684 [==============================] - 19s 2ms/step - loss: 119.6006\n",
            "Epoch 13/50\n",
            "8684/8684 [==============================] - 21s 2ms/step - loss: 109.0674\n",
            "Epoch 14/50\n",
            "8684/8684 [==============================] - 22s 3ms/step - loss: 160.8091\n",
            "Epoch 15/50\n",
            "8684/8684 [==============================] - 21s 2ms/step - loss: 115.1007\n",
            "Epoch 16/50\n",
            "8684/8684 [==============================] - 19s 2ms/step - loss: 132.8950\n",
            "Epoch 17/50\n",
            "8684/8684 [==============================] - 21s 2ms/step - loss: 89.2282\n",
            "Epoch 18/50\n",
            "8684/8684 [==============================] - 19s 2ms/step - loss: 99.0040\n",
            "Epoch 19/50\n",
            "8684/8684 [==============================] - 21s 2ms/step - loss: 111.6863\n",
            "Epoch 20/50\n",
            "8684/8684 [==============================] - 21s 2ms/step - loss: 99.2428\n",
            "Epoch 21/50\n",
            "8684/8684 [==============================] - 21s 2ms/step - loss: 122.2185\n",
            "Epoch 22/50\n",
            "8684/8684 [==============================] - 19s 2ms/step - loss: 139.8590\n",
            "Epoch 23/50\n",
            "8684/8684 [==============================] - 21s 2ms/step - loss: 82.3041\n",
            "Epoch 24/50\n",
            "8684/8684 [==============================] - 19s 2ms/step - loss: 92.7668\n",
            "Epoch 25/50\n",
            "8684/8684 [==============================] - 21s 2ms/step - loss: 97.2094\n",
            "Epoch 26/50\n",
            "8684/8684 [==============================] - 20s 2ms/step - loss: 284.0243\n",
            "Epoch 27/50\n",
            "8684/8684 [==============================] - 21s 2ms/step - loss: 84.7902\n",
            "Epoch 28/50\n",
            "8684/8684 [==============================] - 20s 2ms/step - loss: 74.8892\n",
            "Epoch 29/50\n",
            "8684/8684 [==============================] - 20s 2ms/step - loss: 69.7737\n",
            "Epoch 30/50\n",
            "8684/8684 [==============================] - 21s 2ms/step - loss: 111.7645\n",
            "Epoch 31/50\n",
            "8684/8684 [==============================] - 21s 2ms/step - loss: 73.1741\n",
            "Epoch 32/50\n",
            "8684/8684 [==============================] - 21s 2ms/step - loss: 66.3081\n",
            "Epoch 33/50\n",
            "8684/8684 [==============================] - 20s 2ms/step - loss: 101.9598\n",
            "Epoch 34/50\n",
            "8684/8684 [==============================] - 19s 2ms/step - loss: 170.1842\n",
            "Epoch 35/50\n",
            "8684/8684 [==============================] - 21s 2ms/step - loss: 74.4317\n",
            "Epoch 36/50\n",
            "8684/8684 [==============================] - 19s 2ms/step - loss: 93.6731\n",
            "Epoch 37/50\n",
            "8684/8684 [==============================] - 21s 2ms/step - loss: 65.9481\n",
            "Epoch 38/50\n",
            "8684/8684 [==============================] - 23s 3ms/step - loss: 157.1938\n",
            "Epoch 39/50\n",
            "8684/8684 [==============================] - 21s 2ms/step - loss: 136.8644\n",
            "Epoch 40/50\n",
            "8684/8684 [==============================] - 19s 2ms/step - loss: 68.8842\n",
            "Epoch 41/50\n",
            "8684/8684 [==============================] - 21s 2ms/step - loss: 67.0007\n",
            "Epoch 42/50\n",
            "8684/8684 [==============================] - 20s 2ms/step - loss: 77.0285\n",
            "Epoch 43/50\n",
            "8684/8684 [==============================] - 21s 2ms/step - loss: 88.7294\n",
            "Epoch 44/50\n",
            "8684/8684 [==============================] - 21s 2ms/step - loss: 76.1631\n",
            "Epoch 45/50\n",
            "8684/8684 [==============================] - 21s 2ms/step - loss: 65.9746\n",
            "Epoch 46/50\n",
            "8684/8684 [==============================] - 21s 2ms/step - loss: 154.1247\n",
            "Epoch 47/50\n",
            "8684/8684 [==============================] - 22s 3ms/step - loss: 78.7756\n",
            "Epoch 48/50\n",
            "8684/8684 [==============================] - 23s 3ms/step - loss: 86.5025\n",
            "Epoch 49/50\n",
            "8684/8684 [==============================] - 25s 3ms/step - loss: 93.9547\n",
            "Epoch 50/50\n",
            "8684/8684 [==============================] - 20s 2ms/step - loss: 80.5721\n",
            "2171/2171 [==============================] - 3s 2ms/step\n",
            "Mean Squared Error: 58.411093122791165\n",
            "Root Mean Squared Error: 7.6427150360844385\n",
            "Euclidian distance Error: 7.099756418083934\n"
          ]
        }
      ],
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(2, activation='linear'))\n",
        "\n",
        "# make the step of adam optimizer more smaller\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, epochs=50, batch_size=32, verbose=1)  # Adjust epochs and batch_size as needed\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = model.predict(X_test)\n",
        "mse = mean_squared_error(Y_test, predictions)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "rmse = sqrt(mse)\n",
        "print(f\"Root Mean Squared Error: {rmse}\")\n",
        "euclidian= euclidean_distance_error(Y_test, predictions)\n",
        "print(f\"Euclidian distance Error: {euclidian}\")\n",
        "\n",
        "# Save the model for later use if needed\n",
        "# model.save('your_model.h5')\n"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-16T16:20:30.261214Z",
          "start_time": "2024-01-16T16:14:33.910100Z"
        },
        "id": "205e27932d5cd44b",
        "outputId": "1515122e-55a2-404a-800f-21aeeafa579a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "205e27932d5cd44b",
      "execution_count": 54
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape the data to add a third dimension\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))"
      ],
      "metadata": {
        "id": "rLIW8PwIngHB"
      },
      "id": "rLIW8PwIngHB",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_45 (Conv1D)          (None, 47, 128)           384       \n",
            "                                                                 \n",
            " batch_normalization_42 (Ba  (None, 47, 128)           512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_21 (Dropout)        (None, 47, 128)           0         \n",
            "                                                                 \n",
            " max_pooling1d_33 (MaxPooli  (None, 15, 128)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_46 (Conv1D)          (None, 14, 64)            16448     \n",
            "                                                                 \n",
            " batch_normalization_43 (Ba  (None, 14, 64)            256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling1d_34 (MaxPooli  (None, 4, 64)             0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_47 (Conv1D)          (None, 3, 64)             8256      \n",
            "                                                                 \n",
            " batch_normalization_44 (Ba  (None, 3, 64)             256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv1d_48 (Conv1D)          (None, 2, 32)             4128      \n",
            "                                                                 \n",
            " batch_normalization_45 (Ba  (None, 2, 32)             128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling1d_35 (MaxPooli  (None, 1, 32)             0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " flatten_14 (Flatten)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 128)               4224      \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_59 (Dense)            (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 44994 (175.76 KB)\n",
            "Trainable params: 44418 (173.51 KB)\n",
            "Non-trainable params: 576 (2.25 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "1250/1250 [==============================] - 30s 15ms/step - loss: 67.1495 - mae: 44.2369\n",
            "Epoch 2/50\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 24.1076 - mae: 15.3876\n",
            "Epoch 3/50\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 20.0708 - mae: 12.8153\n",
            "Epoch 4/50\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 16.9868 - mae: 10.8355\n",
            "Epoch 5/50\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 15.3863 - mae: 9.8149\n",
            "Epoch 6/50\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 14.4298 - mae: 9.2092\n",
            "Epoch 7/50\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 13.4938 - mae: 8.6069\n",
            "Epoch 8/50\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 13.1420 - mae: 8.3932\n",
            "Epoch 9/50\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 12.8106 - mae: 8.1711\n",
            "Epoch 10/50\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 12.4409 - mae: 7.9421\n",
            "Epoch 11/50\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 12.4262 - mae: 7.9347\n",
            "Epoch 12/50\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 12.0343 - mae: 7.6758\n",
            "Epoch 13/50\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 11.8937 - mae: 7.5907\n",
            "Epoch 14/50\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 11.8325 - mae: 7.5489\n",
            "Epoch 15/50\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 11.5857 - mae: 7.4009\n",
            "Epoch 16/50\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 11.3405 - mae: 7.2445\n",
            "Epoch 17/50\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 11.2765 - mae: 7.1985\n",
            "Epoch 18/50\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 11.2350 - mae: 7.1707\n",
            "Epoch 19/50\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 11.2218 - mae: 7.1631\n",
            "Epoch 20/50\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 10.8321 - mae: 6.9148\n",
            "Epoch 21/50\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 10.9632 - mae: 7.0024\n",
            "Epoch 22/50\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 10.9899 - mae: 7.0242\n",
            "Epoch 23/50\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 10.6950 - mae: 6.8267\n",
            "Epoch 24/50\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 10.7223 - mae: 6.8532\n",
            "Epoch 25/50\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 10.6505 - mae: 6.8034\n",
            "Epoch 26/50\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 10.5924 - mae: 6.7700\n",
            "Epoch 27/50\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 10.6526 - mae: 6.8045\n",
            "Epoch 28/50\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 10.5341 - mae: 6.7328\n",
            "Epoch 29/50\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 10.4096 - mae: 6.6457\n",
            "Epoch 30/50\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 10.2510 - mae: 6.5517\n",
            "Epoch 31/50\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 10.4113 - mae: 6.6519\n",
            "Epoch 32/50\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 10.2286 - mae: 6.5300\n",
            "Epoch 33/50\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 10.1981 - mae: 6.5114\n",
            "Epoch 34/50\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 10.1177 - mae: 6.4613\n",
            "Epoch 35/50\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 10.1877 - mae: 6.5018\n",
            "Epoch 36/50\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 10.1889 - mae: 6.5119\n",
            "Epoch 37/50\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 10.0318 - mae: 6.4050\n",
            "Epoch 38/50\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 10.0382 - mae: 6.4106\n",
            "Epoch 39/50\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 10.0462 - mae: 6.4214\n",
            "Epoch 40/50\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 9.9383 - mae: 6.3513\n",
            "Epoch 41/50\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 9.8483 - mae: 6.2933\n",
            "Epoch 42/50\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 9.7723 - mae: 6.2487\n",
            "Epoch 43/50\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 9.8268 - mae: 6.2788\n",
            "Epoch 44/50\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 9.8598 - mae: 6.2962\n",
            "Epoch 45/50\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 9.8005 - mae: 6.2661\n",
            "Epoch 46/50\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 9.7375 - mae: 6.2169\n",
            "Epoch 47/50\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 9.8032 - mae: 6.2600\n",
            "Epoch 48/50\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 9.6597 - mae: 6.1713\n",
            "Epoch 49/50\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 9.6846 - mae: 6.1897\n",
            "Epoch 50/50\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 9.6456 - mae: 6.1559\n",
            "313/313 [==============================] - 1s 3ms/step\n",
            "Mean Squared Error: 373.62536043448813\n",
            "Root Mean Squared Error: 19.32939110356268\n",
            "Euclidian distance Error: 18.84619413318489\n"
          ]
        }
      ],
      "source": [
        "model = Sequential([\n",
        "    Conv1D(filters=128, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    MaxPooling1D(pool_size=3),\n",
        "\n",
        "\n",
        "    Conv1D(filters=64, kernel_size=2, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(pool_size=3),\n",
        "\n",
        "    Conv1D(filters=64, kernel_size=2, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Conv1D(filters=32, kernel_size=2, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation= 'relu'),\n",
        "    Dense(64, activation= 'relu'),\n",
        "    Dense(32, activation= 'relu'),\n",
        "    Dense(2, activation='linear')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss=euclidean_distance_loss,  # Using Mean Squared Error\n",
        "              metrics=['mae'])  # Mean Absolute Error as metric\n",
        "\n",
        "# Summary of the Model\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = model.predict(X_test)\n",
        "mse = mean_squared_error(Y_test, predictions)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "rmse = sqrt(mse)\n",
        "print(f\"Root Mean Squared Error: {rmse}\")\n",
        "euclidian= euclidean_distance_error(Y_test, predictions)\n",
        "print(f\"Euclidian distance Error: {euclidian}\")"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-16T14:28:58.246699Z",
          "start_time": "2024-01-16T14:28:58.202379Z"
        },
        "id": "c83a669c1d66ae95",
        "outputId": "cddf6a08-9eb8-4ae7-d232-3d1fe34f9daf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "c83a669c1d66ae95",
      "execution_count": 26
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-14T20:05:45.843897Z",
          "start_time": "2024-01-14T20:05:45.836462Z"
        },
        "id": "edd0373f2cee5d91"
      },
      "id": "edd0373f2cee5d91",
      "execution_count": 8
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}